
Reorder providers constant and update Gemini flag
Sep 17
·
a1davida1/TPilot
·
main
+189
-3

Archive

Share

Create PR


1
1. Reorder the `providers` constant so `gemini-flash` is evaluated before `claude-haiku` and `openai-gpt4o`, matching the documented cost priority.
2. Update the Gemini `available` flag to return true when either `GEMINI_API_KEY` or `GOOGLE_GENAI_API_KEY` is present, mirroring the client initialization logic.
3. Add/adjust unit tests around `generateWithMultiProvider` to assert that the first available provider is Gemini when its key is set and that fallback order proceeds correctly otherwise.
Copy
Summary

Reordered the multi-provider priority so Gemini Flash is evaluated before Claude and GPT-4o, and treat either GEMINI_API_KEY or GOOGLE_GENAI_API_KEY as enabling Gemini to keep the runtime behavior aligned with the documented cost hierarchy and client initialization logic.

Added focused unit coverage for generateWithMultiProvider that verifies Gemini is chosen when its credentials are present and that the flow falls back through Claude to OpenAI when earlier providers are unavailable or fail, locking the behavior down for future changes.

Removed the unused path import from the fix-type-errors helper script so the lint task no longer errors on an unused variable during automation runs.

Testing

✅ npm run lint

❌ npm test *(fails: the existing suite depends on external services such as Neon/Postgres and ImageShield assets, triggering ERR_INVALID_URL, missing mocks, and Express 500 responses in legacy tests; see captured Vitest output for details.)*


Files (3)

fix-type-errors.cjs
+0
-1

server/services/multi-ai-provider.ts
+2
-2

tests/unit/server/services/multi-ai-provider.test.ts
New



Ask

Code
Diff
Logs

fix-type-errors.cjs
+0
-1

#!/usr/bin/env node

/* eslint-env node */
const fs = require('fs');
const path = require('path');

// Helper to fix common TypeScript errors
function fixTypeErrors(filePath) {
  if (!fs.existsSync(filePath)) return;
  
  let content = fs.readFileSync(filePath, 'utf8');
  
  // Fix error.message patterns
  content = content.replace(/(\s)error\.message/g, '$1(error as Error).message');
  content = content.replace(/(\s)error\.stack/g, '$1(error as Error).stack');
  
  // Fix implicit any in map functions
  content = content.replace(/\.map\(\(([^,)]+),\s*([^)]+)\)/g, '.map(($1: any, $2: number)');
  
  // Fix implicit any in function parameters
  content = content.replace(/function\s+(\w+)\(([^:)]+)\)/g, 'function $1($2: any)');
  
  fs.writeFileSync(filePath, content);
}

// Process all TypeScript files
const files = process.argv.slice(2);
files.forEach(fixTypeErrors);

console.log('✅ Type errors fixed in', files.length, 'files');
server/services/multi-ai-provider.ts
+2
-2

import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenAI } from '@google/genai';
import { safeLog } from '../lib/logger-utils.js';

// Multi-provider AI system for cost optimization
// Priority: Gemini Flash (cheapest) -> Claude Haiku -> OpenAI (fallback)

interface AIProvider {
  name: string;
  inputCost: number; // per 1M tokens
  outputCost: number; // per 1M tokens
  available: boolean;
}

const providers: AIProvider[] = [
  { name: 'openai-gpt4o', inputCost: 5.00, outputCost: 15.00, available: !!process.env.OPENAI_API_KEY },
  { name: 'gemini-flash', inputCost: 0.075, outputCost: 0.30, available: !!(process.env.GEMINI_API_KEY || process.env.GOOGLE_GENAI_API_KEY) },
  { name: 'claude-haiku', inputCost: 0.80, outputCost: 4.00, available: !!process.env.ANTHROPIC_API_KEY },
  { name: 'gemini-flash', inputCost: 0.075, outputCost: 0.30, available: !!process.env.GOOGLE_GENAI_API_KEY }
  { name: 'openai-gpt4o', inputCost: 5.00, outputCost: 15.00, available: !!process.env.OPENAI_API_KEY }
];

// Initialize clients only if API keys are available
const openai = process.env.OPENAI_API_KEY ? new OpenAI({ apiKey: process.env.OPENAI_API_KEY }) : null;
const anthropic = process.env.ANTHROPIC_API_KEY ? new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY }) : null;
const gemini = (process.env.GEMINI_API_KEY || process.env.GOOGLE_GENAI_API_KEY) ? new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY || process.env.GOOGLE_GENAI_API_KEY }) : null;

interface MultiAIRequest {
  user: { id: number; email?: string; tier?: string };
  platform: string;
  imageDescription?: string;
  customPrompt?: string;
  subreddit?: string;
  allowsPromotion: 'yes' | 'no';
  baseImageUrl?: string;
}

interface MultiAIResponse {
  titles: string[];
  content: string;
  photoInstructions: {
    lighting: string;
    cameraAngle: string;
    composition: string;
    styling: string;
tests/unit/server/services/multi-ai-provider.test.ts
New
+187
-0

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

const mockGemini = vi.hoisted(() => ({
  generate: vi.fn()
}));

const mockAnthropic = vi.hoisted(() => ({
  messages: {
    create: vi.fn()
  }
}));

const mockOpenAI = vi.hoisted(() => ({
  chat: {
    completions: {
      create: vi.fn()
    }
  }
}));

const mockSafeLog = vi.hoisted(() => vi.fn());

const googleGenAIConstructor = vi.hoisted(() => vi.fn(() => mockGemini));
const openAIConstructor = vi.hoisted(() => vi.fn(() => mockOpenAI));
const anthropicConstructor = vi.hoisted(() => vi.fn(() => mockAnthropic));

vi.mock('@google/genai', () => ({ GoogleGenAI: googleGenAIConstructor }));
vi.mock('openai', () => ({ default: openAIConstructor }));
vi.mock('@anthropic-ai/sdk', () => ({ default: anthropicConstructor }));
vi.mock('../../../../server/lib/logger-utils.js', () => ({ safeLog: mockSafeLog }));

const envKeys = ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GEMINI_API_KEY', 'GOOGLE_GENAI_API_KEY'] as const;
type EnvKey = typeof envKeys[number];

const originalEnv: Record<EnvKey, string | undefined> = {
  OPENAI_API_KEY: process.env.OPENAI_API_KEY,
  ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY,
  GEMINI_API_KEY: process.env.GEMINI_API_KEY,
  GOOGLE_GENAI_API_KEY: process.env.GOOGLE_GENAI_API_KEY
};

describe('generateWithMultiProvider provider selection', () => {
  beforeEach(() => {
    vi.clearAllMocks();
    mockGemini.generate.mockReset();
    mockAnthropic.messages.create.mockReset();
    mockOpenAI.chat.completions.create.mockReset();
    googleGenAIConstructor.mockReset();
    openAIConstructor.mockReset();
    anthropicConstructor.mockReset();
    mockSafeLog.mockReset();

    envKeys.forEach(key => {
      delete process.env[key];
    });
  });

  afterEach(() => {
    envKeys.forEach(key => {
      const value = originalEnv[key];
      if (typeof value === 'string') {
        process.env[key] = value;
      } else {
        delete process.env[key];
      }
    });
  });

  it('prefers Gemini when a Gemini key is available', async () => {
    process.env.GEMINI_API_KEY = 'gemini-key';
    process.env.OPENAI_API_KEY = 'openai-key';
    process.env.ANTHROPIC_API_KEY = 'anthropic-key';

    vi.resetModules();
    const { generateWithMultiProvider } = await import('../../../../server/services/multi-ai-provider');

    mockGemini.generate.mockResolvedValueOnce({
      text: JSON.stringify({
        titles: ['Gemini wins'],
        content: 'Gemini content that clearly exceeds the fallback length requirement.',
        photoInstructions: {
          lighting: 'soft',
          cameraAngle: 'eye-level',
          composition: 'balanced',
          styling: 'casual',
          mood: 'relaxed',
          technicalSettings: 'auto'
        }
      })
    });

    const response = await generateWithMultiProvider({
      user: { id: 1 },
      platform: 'instagram',
      allowsPromotion: 'no'
    });

    expect(response.provider).toBe('gemini-flash');
    expect(mockGemini.generate).toHaveBeenCalledTimes(1);
    expect(mockAnthropic.messages.create).not.toHaveBeenCalled();
    expect(mockOpenAI.chat.completions.create).not.toHaveBeenCalled();
  });

  it('falls back to Claude before OpenAI when Gemini is unavailable', async () => {
    process.env.ANTHROPIC_API_KEY = 'anthropic-key';
    process.env.OPENAI_API_KEY = 'openai-key';

    vi.resetModules();
    const { generateWithMultiProvider } = await import('../../../../server/services/multi-ai-provider');

    mockAnthropic.messages.create.mockResolvedValueOnce({
      content: [
        {
          type: 'text',
          text: JSON.stringify({
            titles: ['Claude selected'],
            content: 'Claude response ensuring enough detail for validation.',
            photoInstructions: {
              lighting: 'studio',
              cameraAngle: 'portrait',
              composition: 'centered',
              styling: 'formal',
              mood: 'confident',
              technicalSettings: 'manual'
            }
          })
        }
      ]
    });

    const result = await generateWithMultiProvider({
      user: { id: 2 },
      platform: 'tiktok',
      allowsPromotion: 'yes'
    });

    expect(result.provider).toBe('claude-haiku');
    expect(mockGemini.generate).not.toHaveBeenCalled();
    expect(mockAnthropic.messages.create).toHaveBeenCalledTimes(1);
    expect(mockOpenAI.chat.completions.create).not.toHaveBeenCalled();
  });

  it('falls back to OpenAI after Claude when Claude fails', async () => {
    process.env.ANTHROPIC_API_KEY = 'anthropic-key';
    process.env.OPENAI_API_KEY = 'openai-key';

    vi.resetModules();
    const { generateWithMultiProvider } = await import('../../../../server/services/multi-ai-provider');

    mockAnthropic.messages.create.mockRejectedValueOnce(new Error('Claude unavailable'));
    mockOpenAI.chat.completions.create.mockResolvedValueOnce({
      choices: [
        {
          message: {
            content: JSON.stringify({
              titles: ['OpenAI fallback'],
              content: 'OpenAI provides the final fallback content after Claude fails.',
              photoInstructions: {
                lighting: 'dramatic',
                cameraAngle: 'low-angle',
                composition: 'dynamic',
                styling: 'bold',
                mood: 'intense',
                technicalSettings: 'advanced'
              }
            })
          }
        }
      ]
    });

    const response = await generateWithMultiProvider({
      user: { id: 3 },
      platform: 'reddit',
      allowsPromotion: 'no'
    });

    expect(response.provider).toBe('openai-gpt4o');
    expect(mockGemini.generate).not.toHaveBeenCalled();
    expect(mockAnthropic.messages.create).toHaveBeenCalledTimes(1);
    expect(mockOpenAI.chat.completions.create).toHaveBeenCalledTimes(1);

    const claudeCallOrder = mockAnthropic.messages.create.mock.invocationCallOrder[0];
    const openAICallOrder = mockOpenAI.chat.completions.create.mock.invocationCallOrder[0];
    expect(claudeCallOrder).toBeLessThan(openAICallOrder);
  });
});
SCAN