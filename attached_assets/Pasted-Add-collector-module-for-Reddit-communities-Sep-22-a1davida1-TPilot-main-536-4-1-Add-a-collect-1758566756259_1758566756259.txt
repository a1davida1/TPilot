Add collector module for Reddit communities
Sep 22 ¬∑ a1davida1/TPilot ¬∑ main
+536-4
1. Add a collector module under `server/scripts/` (for example `sync-reddit-communities.ts`) that authenticates with Snoowrap the same way `RedditManager` does, but using app-level credentials rather than per-user tokens (`server/lib/reddit.ts` shows the required env vars and Snoowrap constructor).
2. For each target subreddit (seed an array or read from the UI-configured list), call `getSubreddit(name).fetch()` to capture subscriber counts, NSFW flags, and capability booleans; enrich with calculated engagement metrics if you have historical performance.
3. Upsert the result into `redditCommunities` using `db.insert(...).onConflictDoUpdate(...)`, matching the shapes expected by `insertRedditCommunitySchema` in `server/reddit-communities.ts`.
4. Register the script with your job runner/cron (e.g., a BullMQ worker or a simple `npm run` entry) so metadata refreshes daily.

Summary

    Added a Snoowrap-powered sync-reddit-communities collector that validates environment configuration, normalizes subreddit data, computes engagement heuristics, and upserts rows via conflict-aware inserts with a CLI entry point for manual runs.

Introduced a CommunitySyncWorker that registers the new queue, safely skips scheduling in test environments, logs lifecycle events, and schedules daily refresh jobs while guarding against queue connectivity issues.

Extended the queue system and monitoring surface to recognize the community sync queue, including queue naming, worker initialization/shutdown, scaling defaults, health metrics, and test expectations, plus an npm script for manual execution.

Testing

    ‚úÖ npm test -- tests/unit/workers/queue-initialization.test.ts

‚ö†Ô∏è npm run lint (passes with existing repository warnings about unused symbols and any usage)

‚ùå npm test *(fails due to extensive pre-existing suite dependencies on external AI services and missing PostgreSQL URL; new community sync worker now skips scheduling under NODE_ENV=test)*


package.json
+2-1
{
  "name": "rest-express",
  "version": "1.0.0",
  "type": "module",
  "license": "MIT",
  "scripts": {
    "dev": "NODE_ENV=development tsx server/index.ts",
    "build": "bash build-production.sh",
    "prestart": "npm run build",
    "build:client": "vite build",
    "build:server": "tsc -p tsconfig.server.json",
    "start": "NODE_ENV=production node dist/server/index.js",
    "db:generate": "drizzle-kit generate",
    "db:migrate": "drizzle-kit migrate",
    "db:studio": "drizzle-kit studio",
    "typecheck": "tsc -p tsconfig.json --noEmit",
    "fix-imports": "bash fix-all-imports.sh",
    "lint": "eslint .",
    "test": "vitest run",
    "test:coverage": "vitest run --coverage",
    "test:e2e": "tsx vendor/playwright-test/runner.ts"
    "test:e2e": "tsx vendor/playwright-test/runner.ts",
    "sync:reddit-communities": "tsx server/scripts/sync-reddit-communities.ts"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.37.0",
    "@aws-sdk/client-s3": "^3.864.0",
    "@aws-sdk/s3-request-presigner": "^3.864.0",
    "@eslint/js": "^9.35.0",
    "@google-cloud/storage": "^7.17.0",
    "@google/genai": "^1.12.0",
    "@google/generative-ai": "^0.24.1",
    "@hookform/resolvers": "^3.10.0",
    "@jridgewell/trace-mapping": "^0.3.25",
    "@maxmind/geoip2-node": "^6.1.0",
    "@neondatabase/serverless": "^0.10.4",
    "@playwright/test": "file:vendor/playwright-test",
    "@radix-ui/react-accordion": "^1.2.4",
    "@radix-ui/react-alert-dialog": "^1.1.7",
    "@radix-ui/react-aspect-ratio": "^1.1.3",
    "@radix-ui/react-avatar": "^1.1.4",
    "@radix-ui/react-checkbox": "^1.1.5",
    "@radix-ui/react-collapsible": "^1.1.4",
    "@radix-ui/react-context-menu": "^2.2.7",
    "@radix-ui/react-dialog": "^1.1.7",
    "@radix-ui/react-dropdown-menu": "^2.1.7",
    "@radix-ui/react-hover-card": "^1.1.7",
    "@radix-ui/react-label": "^2.1.3",
server/lib/queue-monitor.ts
+1-0
@@ -115,50 +115,51 @@ export class QueueMonitor {
        // Set error state
        this.metrics.set(queueName, {
          pending: 0,
          active: 0,
          completed: 0,
          failed: 0,
          delayed: 0,
          failureRate: 1.0, // 100% failure rate indicates error
          throughput: 0,
          avgProcessingTime: 0,
          healthStatus: 'critical',
        });
      }
    }

    // Update worker metrics
    await this.collectWorkerMetrics();
  }

  private async collectWorkerMetrics() {
    const workers = [
      { name: 'post-worker', concurrency: 2 },
      { name: 'metrics-worker', concurrency: 3 },
      { name: 'ai-promo-worker', concurrency: 1 },
      { name: 'dunning-worker', concurrency: 1 },
      { name: 'community-sync-worker', concurrency: 1 },
    ];

    for (const worker of workers) {
      try {
        // In a full implementation, this would collect actual worker stats
        const existing = this.workerMetrics.get(worker.name);
        const uptime = existing ? existing.uptime + 30000 : 30000; // Add 30s

        const workerMetrics: WorkerMetrics = {
          name: worker.name,
          status: 'running',
          concurrency: worker.concurrency,
          processed: (existing?.processed || 0) + Math.floor(Math.random() * 5), // Mock data
          failed: existing?.failed || 0,
          uptime,
          memoryUsage: process.memoryUsage().heapUsed / worker.concurrency, // Rough estimate
          lastActivity: new Date(),
        };

        this.workerMetrics.set(worker.name, workerMetrics);

      } catch (error) {
        console.error(`Failed to collect worker metrics for ${worker.name}:`, error);
      }
    }
server/lib/queue/index.ts
+7-1
/**
 * Phase 5: Modern Queue System
 * Uses abstracted queue interface with Redis/PostgreSQL fallback
 */

import { getQueueBackend, enqueue, registerProcessor } from "../queue-factory.js";
import type { Platform, PostContent } from "../../social-media/social-media-manager.js";

// Queue names for type safety
export const QUEUE_NAMES = {
  POST: 'post-queue',
  BATCH_POST: 'batch-post-queue',
  METRICS: 'metrics-queue', 
  METRICS: 'metrics-queue',
  AI_PROMO: 'ai-promo-queue',
  DUNNING: 'dunning-queue',
  COMMUNITY_SYNC: 'community-sync-queue',
} as const;

export type QueueNames = typeof QUEUE_NAMES[keyof typeof QUEUE_NAMES];

// Job data types
export interface PostJobData {
  userId: number;
  // Existing Reddit-specific fields
  postJobId?: number;
  subreddit?: string;
  titleFinal?: string;
  bodyFinal?: string;
  mediaKey?: string;
  // New social media fields
  platforms?: Platform[];
  content?: PostContent;
}

export interface BatchPostJobData {
  userId: number;
  campaignId: string;
  subreddits: string[];
  titleTemplate: string;
  bodyTemplate: string;
  mediaKey?: string;
  delayBetweenPosts?: number;
}

export interface MetricsJobData {
  postJobId: number;
  redditPostId: string;
  scheduledFor: Date;
}

export interface AiPromoJobData {
  userId: number;
  generationId: number;
  promptText?: string;
  imageKey?: string;
  platforms: string[];
  styleHints?: string[];
  variants?: number;
}

export interface DunningJobData {
  subscriptionId: number;
  attempt: number;
  maxAttempts?: number;
}

export interface CommunitySyncJobData {
  subreddits?: string[];
  triggeredBy?: string;
}

// Helper function to add jobs with proper types
export async function addJob<T>(
  queueName: QueueNames,
  jobData: T,
  options?: {
    delay?: number;
    priority?: number;
    attempts?: number;
  }
) {
  return enqueue(queueName, jobData, options);
}

// Queue management functions
export async function pauseQueue(queueName: string): Promise<void> {
  const backend = getQueueBackend();
  if (backend.pause) {
    await backend.pause(queueName);
  } else {
    console.warn(`Queue backend does not support pausing queue: ${queueName}`);
  }
}

export async function resumeQueue(queueName: string): Promise<void> {
  const backend = getQueueBackend();
server/lib/worker-scaler.ts
+4-0
@@ -31,50 +31,54 @@ export class WorkerScaler {
    scaleDownThreshold: 300000, // Scale down after 5 minutes of low activity
    cooldownPeriod: 120000, // 2 minutes between scaling actions
  };

  private queueConfigs: Record<string, Partial<ScalingConfig>> = {
    [QUEUE_NAMES.POST]: {
      maxConcurrency: 5, // Respect Reddit rate limits
      scaleUpThreshold: 10,
    },
    [QUEUE_NAMES.METRICS]: {
      maxConcurrency: 8, // Metrics can be more concurrent
      scaleUpThreshold: 30,
    },
    [QUEUE_NAMES.AI_PROMO]: {
      maxConcurrency: 2, // AI generation is expensive
      scaleUpThreshold: 5,
    },
    [QUEUE_NAMES.DUNNING]: {
      maxConcurrency: 2, // Payment processing should be conservative
      scaleUpThreshold: 8,
    },
    [QUEUE_NAMES.BATCH_POST]: {
      maxConcurrency: 1, // Always single concurrency for batch posts
      scaleUpThreshold: 999, // Never scale up
    },
    [QUEUE_NAMES.COMMUNITY_SYNC]: {
      maxConcurrency: 1,
      scaleUpThreshold: 2,
    },
  };

  public static getInstance(): WorkerScaler {
    if (!WorkerScaler.instance) {
      WorkerScaler.instance = new WorkerScaler();
    }
    return WorkerScaler.instance;
  }

  async startScaling(intervalMs: number = 60000) { // Check every minute
    if (this.scaling) return;

    logger.info('üìà Starting worker auto-scaling...');
    this.scaling = true;

    // Initialize scaling states
    for (const queueName of Object.values(QUEUE_NAMES)) {
      this.scalingStates.set(queueName, {
        currentConcurrency: this.getQueueConfig(queueName).minConcurrency,
        targetConcurrency: this.getQueueConfig(queueName).minConcurrency,
        pendingJobs: 0,
        isScaling: false,
      });
    }

server/lib/workers/community-sync-worker.ts
New
+105-0
import { z } from 'zod';
import { registerProcessor } from '../queue-factory.js';
import { addJob, QUEUE_NAMES, type CommunitySyncJobData } from '../queue/index.js';
import { logger } from '../logger.js';
import { syncRedditCommunities } from '../../scripts/sync-reddit-communities.js';

const communitySyncJobSchema = z.object({
  subreddits: z.array(z.string()).optional(),
  triggeredBy: z.string().optional(),
});

const DAY_IN_MS = 24 * 60 * 60 * 1000;

export class CommunitySyncWorker {
  private initialized = false;
  
  private canScheduleJobs(): boolean {
    return process.env.NODE_ENV !== 'test';
  }

  async initialize(): Promise<void> {
    if (this.initialized) {
      return;
    }

    await registerProcessor<CommunitySyncJobData>(
      QUEUE_NAMES.COMMUNITY_SYNC,
      this.processJob.bind(this),
      { concurrency: 1 }
    );

    await this.enqueueInitialSync();

    this.initialized = true;
    logger.info('‚úÖ Community sync worker initialized');
  }

  private async enqueueInitialSync(): Promise<void> {
    if (!this.canScheduleJobs()) {
      logger.info('Community sync worker scheduling skipped in test environment');
      return;
    }

    try {
      await addJob(
        QUEUE_NAMES.COMMUNITY_SYNC,
        { triggeredBy: 'startup' },
      );
    } catch (error) {
      logger.warn('Community sync worker could not enqueue initial job', {
        error: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  }

  private async scheduleNextRun(): Promise<void> {
    if (!this.canScheduleJobs()) {
      return;
    }

    try {
      await addJob(
        QUEUE_NAMES.COMMUNITY_SYNC,
        { triggeredBy: 'schedule' },
        { delay: DAY_IN_MS }
      );
    } catch (error) {
      logger.warn('Community sync worker could not schedule next run', {
        error: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  }

  private async processJob(jobData: unknown, jobId: string): Promise<void> {
    const payload = communitySyncJobSchema.parse(jobData ?? {});

    try {
      const result = await syncRedditCommunities({
        subreddits: payload.subreddits,
        runId: jobId,
      });

      logger.info('Community sync job completed', {
        jobId,
        processed: result.processed,
        succeeded: result.succeeded,
        failed: result.failed,
      });
    } catch (error) {
      logger.error('Community sync job failed', {
        jobId,
        error: error instanceof Error ? error.message : 'Unknown error',
      });
      throw error instanceof Error ? error : new Error(String(error));
    } finally {
      await this.scheduleNextRun();
    }
  }

  async close(): Promise<void> {
    this.initialized = false;
  }
}

export const communitySyncWorker = new CommunitySyncWorker();
server/lib/workers/index.ts
+10-0
import { postWorker } from "./post-worker.js";
import { metricsWorker } from "./metrics-worker.js";
import { aiPromoWorker } from "./ai-promo-worker.js";
import { dunningWorker } from "./dunning-worker.js";
import { batchPostingWorker } from "./batch-posting-worker.js";
import { communitySyncWorker } from "./community-sync-worker.js";
import { logger } from "../logger.js";

// Initialize all workers
export async function initializeWorkers() {
  logger.info('üîÑ Initializing background workers...');
  
  // Initialize each worker
  await postWorker.initialize();
  logger.info('‚úÖ Post worker initialized');
  
  await metricsWorker.initialize();
  logger.info('‚úÖ Metrics worker initialized');
  
  await aiPromoWorker.initialize();
  logger.info('‚úÖ AI Promo worker initialized');
  
  await dunningWorker.initialize();
  logger.info('‚úÖ Dunning worker initialized');
  
  await batchPostingWorker.initialize();
  logger.info('‚úÖ Batch posting worker initialized');

  await communitySyncWorker.initialize();
  logger.info('‚úÖ Community sync worker initialized');

  const { queueMonitor } = await import("../queue-monitor.js");
  await queueMonitor.startMonitoring(30000);
  logger.info('üöÄ Queue monitoring started');
}

// Graceful shutdown
export async function shutdownWorkers() {
  logger.info('üîÑ Shutting down workers...');
  
  await postWorker.close();
  await metricsWorker.close();
  await aiPromoWorker.close();
  await dunningWorker.close();
  await batchPostingWorker.close();
  await communitySyncWorker.close();
  
  logger.info('‚úÖ All workers shut down');
}

// Health check for workers
export function getWorkersHealth() {
  return {
    postWorker: {
      status: 'running', // Would check actual worker status
      processed: 0, // Would get from worker stats
      failed: 0,
    },
    communitySyncWorker: {
      status: 'running',
      processed: 0,
      failed: 0,
    },
  };
}
server/scripts/sync-reddit-communities.ts
New
+404-0
import { fileURLToPath } from 'node:url';
import snoowrap from 'snoowrap';
import type Subreddit from 'snoowrap/dist/objects/Subreddit';
import { db, closeDatabaseConnections } from '../db.js';
import {
  redditCommunities,
  insertRedditCommunitySchema,
  type InsertRedditCommunity,
  type RedditCommunity
} from '@shared/schema';
import { logger } from '../bootstrap/logger.js';

const REQUIRED_ENV_VARS = [
  'REDDIT_CLIENT_ID',
  'REDDIT_CLIENT_SECRET',
  'REDDIT_USER_AGENT',
  'REDDIT_USERNAME',
  'REDDIT_PASSWORD'
] as const;

const DEFAULT_SUBREDDITS = [
  'onlyfansadvice',
  'onlyfansmarketing',
  'fanslycreators',
  'onlyfans101',
  'influencermarketing',
  'instagrammarketing',
  'socialmedia',
  'tiktokmarketing',
  'contentmarketing',
  'marketing'
] as const;

type SnoowrapRule = {
  short_name?: string;
  description?: string;
};

interface SubredditRulesResponse {
  rules: SnoowrapRule[];
  site_rules: string[];
}

export interface SyncRedditCommunitiesOptions {
  subreddits?: string[];
  runId?: string;
}

export interface SyncRedditCommunitiesResult {
  processed: number;
  succeeded: number;
  failed: number;
}

function requireEnvVar(name: typeof REQUIRED_ENV_VARS[number]): string {
  const value = process.env[name];
  if (!value) {
    throw new Error(`Missing required environment variable: ${name}`);
  }
  return value;
}

function normalizeSubredditName(name: string): string {
  return name.replace(/^r\//i, '').trim();
}

function isRecord(value: unknown): value is Record<string, unknown> {
  return typeof value === 'object' && value !== null && !Array.isArray(value);
}

function asStringArray(value: unknown): string[] | undefined {
  if (Array.isArray(value)) {
    const result = value.filter((item): item is string => typeof item === 'string');
    return result;
  }
  return undefined;
}

function computeEngagementRate(subreddit: Subreddit): number {
  const subscribers = typeof subreddit.subscribers === 'number' ? subreddit.subscribers : 0;
  const activeUsers = typeof subreddit.active_user_count === 'number'
    ? subreddit.active_user_count
    : typeof subreddit.accounts_active === 'number'
      ? subreddit.accounts_active
      : 0;

  if (subscribers === 0) {
    return 0;
  }

  const rate = Math.round((activeUsers / subscribers) * 100);
  return Math.min(Math.max(rate, 0), 100);
}

function computeSuccessProbability(engagementRate: number, subscribers: number): number {
  const scaledByEngagement = engagementRate * 1.5;
  const subscriberSignal = Math.log10(Math.max(subscribers, 10)) * 5;
  const probability = Math.round(scaledByEngagement + subscriberSignal);
  return Math.min(Math.max(probability, 20), 95);
}

function computeCompetitionLevel(subscribers: number): 'low' | 'medium' | 'high' {
  if (subscribers >= 1_000_000) {
    return 'high';
  }
  if (subscribers >= 200_000) {
    return 'medium';
  }
  return 'low';
}

function computeGrowthTrend(previousMembers: number | undefined, currentMembers: number): 'up' | 'steady' | 'down' {
  if (previousMembers === undefined || previousMembers === 0) {
    return 'steady';
  }

  const delta = currentMembers - previousMembers;
  const percentChange = (delta / previousMembers) * 100;

  if (percentChange > 2) {
    return 'up';
  }
  if (percentChange < -2) {
    return 'down';
  }
  return 'steady';
}

function computeModActivity(activeUsers: number, subscribers: number): 'low' | 'medium' | 'high' {
  if (subscribers === 0) {
    return 'low';
  }

  const ratio = activeUsers / subscribers;
  if (ratio >= 0.1) {
    return 'high';
  }
  if (ratio >= 0.05) {
    return 'medium';
  }
  return 'low';
}

function computeBestPostingTimes(engagementRate: number, existingTimes?: string[] | null): string[] {
  if (existingTimes && existingTimes.length > 0) {
    return existingTimes;
  }

  if (engagementRate >= 20) {
    return ['Weekdays 15:00 UTC', 'Sundays 18:00 UTC'];
  }
  if (engagementRate >= 10) {
    return ['Weekdays 17:00 UTC', 'Saturdays 16:00 UTC'];
  }
  return ['Weekdays 19:00 UTC'];
}

function buildPostingLimits(subreddit: Subreddit, existingLimits: unknown): Record<string, unknown> {
  const baseLimits = isRecord(existingLimits) ? existingLimits : {};
  return {
    ...baseLimits,
    capabilities: {
      allowImages: subreddit.allow_images,
      allowVideos: subreddit.allow_videos,
      allowGifs: subreddit.allow_videogifs,
      allowSpoilers: subreddit.spoilers_enabled,
      nsfw: subreddit.over18,
      quarantine: subreddit.quarantine,
      subredditType: subreddit.subreddit_type,
    },
  };
}

function buildRules(existingRules: unknown, fetchedRules: SnoowrapRule[]): string[] {
  const rules = new Set<string>();
  const existing = asStringArray(existingRules);
  if (existing) {
    for (const rule of existing) {
      rules.add(rule);
    }
  }

  for (const rule of fetchedRules) {
    const label = rule.short_name ?? rule.description;
    if (label) {
      rules.add(label);
    }
  }

  return Array.from(rules);
}

function collectTags(
  existingTags: string[] | null | undefined,
  category: string,
  subreddit: Subreddit
): string[] {
  const tags = new Set<string>();
  for (const tag of existingTags ?? []) {
    if (typeof tag === 'string' && tag.trim().length > 0) {
      tags.add(tag);
    }
  }

  if (category) {
    tags.add(category.toLowerCase());
  }
  if (subreddit.lang) {
    tags.add(subreddit.lang.toLowerCase());
  }
  if (subreddit.over18) {
    tags.add('nsfw');
  }
  if (subreddit.quarantine) {
    tags.add('quarantined');
  }

  return Array.from(tags);
}

function resolvePromotionPolicy(subreddit: Subreddit, existingPolicy?: string | null): string {
  if (existingPolicy && existingPolicy.length > 0) {
    return existingPolicy;
  }

  switch (subreddit.submission_type) {
    case 'any':
      return 'limited';
    case 'self':
      return 'strict';
    case 'link':
      return 'limited';
    default:
      return 'no';
  }
}

function compactInsertPayload(payload: InsertRedditCommunity): Partial<InsertRedditCommunity> {
  const result: Partial<InsertRedditCommunity> = {};
  const entries = Object.entries(payload) as Array<[
    keyof InsertRedditCommunity,
    InsertRedditCommunity[keyof InsertRedditCommunity]
  ]>;

  for (const [key, value] of entries) {
    if (value !== undefined) {
      result[key] = value;
    }
  }

  return result;
}

async function fetchSubredditRules(subreddit: Subreddit): Promise<SnoowrapRule[]> {
  try {
    const response = await subreddit.getRules() as unknown as SubredditRulesResponse;
    return response?.rules ?? [];
  } catch (error) {
    logger.warn('Failed to fetch subreddit rules', {
      subreddit: subreddit.display_name,
      error: error instanceof Error ? error.message : 'Unknown error',
    });
    return [];
  }
}

export async function syncRedditCommunities(
  options: SyncRedditCommunitiesOptions = {}
): Promise<SyncRedditCommunitiesResult> {
  for (const name of REQUIRED_ENV_VARS) {
    requireEnvVar(name);
  }

  const redditClient = new snoowrap({
    userAgent: requireEnvVar('REDDIT_USER_AGENT'),
    clientId: requireEnvVar('REDDIT_CLIENT_ID'),
    clientSecret: requireEnvVar('REDDIT_CLIENT_SECRET'),
    username: requireEnvVar('REDDIT_USERNAME'),
    password: requireEnvVar('REDDIT_PASSWORD'),
  });

  const existingCommunities = await db.select().from(redditCommunities);
  const existingByName = new Map<string, RedditCommunity>();
  for (const community of existingCommunities) {
    existingByName.set(community.name.toLowerCase(), community);
    existingByName.set(community.id.toLowerCase(), community);
  }

  const targetSubreddits = (
    options.subreddits && options.subreddits.length > 0
      ? options.subreddits
      : existingCommunities.length > 0
        ? existingCommunities.map((community) => community.name)
        : [...DEFAULT_SUBREDDITS]
  ).map((name) => normalizeSubredditName(name));

  let processed = 0;
  let succeeded = 0;
  let failed = 0;

  for (const subredditName of targetSubreddits) {
    processed += 1;
    try {
      logger.info('Syncing subreddit metadata', {
        subreddit: subredditName,
        runId: options.runId,
      });

      const subreddit = await redditClient.getSubreddit(subredditName).fetch();
      const fetchedRules = await fetchSubredditRules(subreddit);

      const subscribers = typeof subreddit.subscribers === 'number' ? subreddit.subscribers : 0;
      const activeUsers = typeof subreddit.active_user_count === 'number'
        ? subreddit.active_user_count
        : typeof subreddit.accounts_active === 'number'
          ? subreddit.accounts_active
          : 0;

      const engagementRate = computeEngagementRate(subreddit);
      const existing = existingByName.get(subredditName.toLowerCase());
      const category = subreddit.advertiser_category
        ? subreddit.advertiser_category.toLowerCase()
        : existing?.category ?? 'general';

      const bestPostingTimes = computeBestPostingTimes(engagementRate, existing?.bestPostingTimes);
      const promotionPolicy = resolvePromotionPolicy(subreddit, existing?.promotionAllowed);
      const rules = buildRules(existing?.rules, fetchedRules);
      const postingLimits = buildPostingLimits(subreddit, existing?.postingLimits);
      const tags = collectTags(existing?.tags, category, subreddit);

      const communityRecord: InsertRedditCommunity = insertRedditCommunitySchema.parse({
        id: existing?.id ?? subreddit.display_name.toLowerCase(),
        name: subreddit.display_name.toLowerCase(),
        displayName: subreddit.title || subreddit.display_name,
        members: subscribers,
        engagementRate,
        category,
        verificationRequired: subreddit.subreddit_type !== 'public' || subreddit.quarantine,
        promotionAllowed: promotionPolicy,
        postingLimits,
        rules,
        bestPostingTimes,
        averageUpvotes: existing?.averageUpvotes ?? Math.max(10, Math.round(engagementRate * 12)),
        successProbability: computeSuccessProbability(engagementRate, subscribers),
        growthTrend: computeGrowthTrend(existing?.members, subscribers),
        modActivity: computeModActivity(activeUsers, subscribers),
        description: subreddit.public_description?.trim() || subreddit.description?.trim() || existing?.description || '',
        tags,
        competitionLevel: computeCompetitionLevel(subscribers),
      }) as InsertRedditCommunity;

      const updatePayload = compactInsertPayload(communityRecord);

      await db
        .insert(redditCommunities)
        .values(communityRecord)
        .onConflictDoUpdate({
          target: redditCommunities.id,
          set: updatePayload,
        });

      succeeded += 1;
      logger.info('Upserted Reddit community metadata', {
        subreddit: subredditName,
        members: subscribers,
        engagementRate,
      });
    } catch (error) {
      failed += 1;
      logger.error('Failed to sync subreddit metadata', {
        subreddit: subredditName,
        runId: options.runId,
        error: error instanceof Error ? error.message : 'Unknown error',
      });
    }
  }

  const result: SyncRedditCommunitiesResult = {
    processed,
    succeeded,
    failed,
  };

  logger.info('Reddit community sync complete', result);
  return result;
}

async function runCli() {
  try {
    await syncRedditCommunities();
    await closeDatabaseConnections();
    process.exit(0);
  } catch (error) {
    logger.error('Reddit community sync failed', {
      error: error instanceof Error ? error.message : 'Unknown error',
    });
    await closeDatabaseConnections();
    process.exit(1);
  }
}

if (process.argv[1] === fileURLToPath(import.meta.url)) {
  runCli();
}
tests/unit/workers/queue-initialization.test.ts
+3-2
@@ -39,54 +39,55 @@ describe('Worker Queue Initialization', () => {
      const hasQueueLog = logSpy.mock.calls.some(([msg]: any[]) => 
        typeof msg === 'string' && msg.includes('queue backend')
      );
      expect(logSpy).toHaveBeenCalled();
    });
  });

  describe('Worker Initialization Logging', () => {
    it('should log successful worker initialization', async () => {
      await initializeWorkers();

      // Should log initialization progress
      const infoCalls = infoSpy.mock.calls.flat();
      const hasWorkerLog = infoCalls.some((call: any) =>
        typeof call === 'string' && call.includes('worker initialized')
      );
      expect(infoSpy).toHaveBeenCalled();
    });

    it('should initialize all required workers', async () => {
      await initializeWorkers();

      // Check that all expected workers are initialized
      const expectedWorkers = [
        'Post worker',
        'Metrics worker', 
        'Metrics worker',
        'AI Promo worker',
        'Dunning worker',
        'Batch posting worker'
        'Batch posting worker',
        'Community sync worker'
      ];

      const infoCalls = infoSpy.mock.calls.flat();
      const loggedText = infoCalls.join(' ');

      expectedWorkers.forEach(workerName => {
        expect(loggedText).toContain(workerName);
      });
    });

    it('should log queue monitoring startup', async () => {
      // This test checks if queue monitoring is properly logged
      await initializeWorkers();

      // Should include queue monitoring messages
      const logCalls = [...logSpy.mock.calls, ...infoSpy.mock.calls].flat();
      const hasMonitoringLog = logCalls.some(call =>
        typeof call === 'string' && 
        (call.includes('queue monitoring') || call.includes('Queue monitoring'))
      );
      expect(hasMonitoringLog).toBe(true);
    });
  });

  describe('Error Handling', () => {
